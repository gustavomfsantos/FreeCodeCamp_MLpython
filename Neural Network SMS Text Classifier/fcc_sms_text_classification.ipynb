{"cells":[{"cell_type":"code","source":["# In this challenge, you need to create a machine learning model that will classify SMS messages as either \"ham\" or \"spam\".\n","# A \"ham\" message is a normal message sent by a friend. A \"spam\" message is an advertisement or a message sent by a company.\n","\n","# You should create a function called predict_message that takes a message string as an argument and returns a list.\n","# The first element in the list should be a number between zero and one that indicates the likeliness of \"ham\" (0) or \"spam\" (1).\n","# The second element in the list should be the word \"ham\" or \"spam\", depending on which is most likely.\n","\n","# For this challenge, you will use the SMS Spam Collection dataset. The dataset has already been grouped into train data and test data.\n","\n","# The first two cells import the libraries and data. The final cell tests your model and function. Add your code in between these cells."],"metadata":{"id":"8EWwv7qU5vX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8RZOuS9LWQvv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696886932396,"user_tz":180,"elapsed":19773,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"bc0654c7-519e-42ff-e505-e5b460c5330c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tf-nightly in /usr/local/lib/python3.10/dist-packages (2.15.0.dev20231009)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.59.0)\n","Requirement already satisfied: tb-nightly~=2.15.0.a in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.15.0a20231009)\n","Requirement already satisfied: tf-estimator-nightly~=2.14.0.dev in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.14.0.dev2023080308)\n","Requirement already satisfied: keras-nightly~=2.15.0.dev in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.15.0.dev2023092207)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.15.0.a->tf-nightly) (3.0.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tb-nightly~=2.15.0.a->tf-nightly) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.15.0.a->tf-nightly) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.15.0.a->tf-nightly) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.15.0.a->tf-nightly) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tb-nightly~=2.15.0.a->tf-nightly) (3.2.2)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n","Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n","Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.23.5)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.5.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2023.7.22)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.60.0)\n","2.15.0-dev20231009\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# import libraries\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  !pip install tf-nightly\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from sklearn.preprocessing import LabelEncoder\n","from keras.preprocessing.text import Tokenizer\n","# You may need to install libraries\n","# ! pip install pandas\n","# ! pip install nltk\n","# ! pip install scikit-learn\n","\n","# Import libraries\n","import string\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lMHwYXHXCar3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696887152454,"user_tz":180,"elapsed":1203,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"969f51c4-c336-4641-a1ac-d8681df37b9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-09 21:32:30--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 104.26.2.33, 172.67.70.149, ...\n","Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 358233 (350K) [text/tab-separated-values]\n","Saving to: ‘train-data.tsv.42’\n","\n","train-data.tsv.42   100%[===================>] 349.84K  --.-KB/s    in 0.06s   \n","\n","2023-10-09 21:32:30 (5.79 MB/s) - ‘train-data.tsv.42’ saved [358233/358233]\n","\n","--2023-10-09 21:32:30--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 104.26.3.33, 104.26.2.33, 172.67.70.149, ...\n","Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|104.26.3.33|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 118774 (116K) [text/tab-separated-values]\n","Saving to: ‘valid-data.tsv.42’\n","\n","valid-data.tsv.42   100%[===================>] 115.99K  --.-KB/s    in 0.03s   \n","\n","2023-10-09 21:32:30 (3.42 MB/s) - ‘valid-data.tsv.42’ saved [118774/118774]\n","\n","Number of spam messages: 560\n","Number of ham messages: 3619\n","(5571, 2)\n","  label                                            message\n","0   ham  ahhhh...just woken up!had a bad dream about u ...\n","1   ham                           you can never do nothing\n","2   ham  now u sound like manky scouse boy steve,like! ...\n","3   ham  mum say we wan to go then go... then she can s...\n","4   ham  never y lei... i v lazy... got wat? dat day ü ...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-a1e376cd6fe3>:29: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  data = train_text.append(test_text, ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":["(4179,)\n","(4179, 150)\n"]}],"source":["# get data files\n","!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","\n","train_file_path = \"train-data.tsv\"\n","test_file_path = \"valid-data.tsv\"\n","\n","train_text = pd.read_csv(train_file_path, sep='\\t', header=None)\n","train_text.columns = ['label', 'message']\n","test_text = pd.read_csv(test_file_path, sep='\\t' ,  header=None)\n","test_text.columns = ['label', 'message']\n","\n","\n","\n","#Check for balance in train set\n","spam_messages = train_text[train_text['label'] == \"spam\"]\n","ham_messages = train_text[train_text['label'] == \"ham\"]\n","print(f\"Number of spam messages: {len(spam_messages)}\")\n","print(f\"Number of ham messages: {len(ham_messages)}\")\n","\n","#Get labels and texts separetely\n","train_data = train_text['message']\n","train_labels = train_text['label']\n","\n","test_data = test_text['message']\n","test_labels = test_text['label']\n","\n","\n","data = train_text.append(test_text, ignore_index=True)\n","print(data.shape)\n","print(data.head())\n","tags = data[\"label\"]\n","texts = data[\"message\"]\n","num_max = 1000\n","le = LabelEncoder()\n","tags = le.fit_transform(tags)\n","tok = Tokenizer(num_words=num_max)\n","tok.fit_on_texts(texts)\n","\n","mat_data = tok.texts_to_matrix(texts,mode='count')\n","mat_train = tok.texts_to_matrix(train_text,mode='count')\n","mat_test = tok.texts_to_matrix(test_text,mode='count')\n","\n","max_len = 150\n","x_train = tok.texts_to_sequences(train_text[\"message\"])\n","x_test = tok.texts_to_sequences(test_text[\"message\"])\n","encoded_train_processed = keras.preprocessing.sequence.pad_sequences(x_train,maxlen=max_len)\n","encoded_test_processed = keras.preprocessing.sequence.pad_sequences(x_test,maxlen=max_len)\n","\n","train_labels_ = train_text[\"label\"]\n","train_labels = le.fit_transform(train_labels_)\n","test_labels_ = test_text[\"label\"]\n","test_labels = le.fit_transform(test_labels_)\n","\n","print(train_labels.shape)\n","print(encoded_train_processed.shape)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"B-ZEfnquMZXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 150\n","BATCH_SIZE = 32\n","VOCAB_SIZE = 5000\n","\n","\n","#Creating the Model\n","model = tf.keras.Sequential([\n","      tf.keras.layers.Embedding(VOCAB_SIZE, 32 ),\n","      tf.keras.layers.LSTM(32),\n","      tf.keras.layers.Dense(1, activation = 'sigmoid')\n","  ])\n","\n","model.summary\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsTXmG0YqQTv","executionInfo":{"status":"ok","timestamp":1696887183450,"user_tz":180,"elapsed":639,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"370b40a0-9f38-4797-d357-3fe1506b97a3"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Model.summary of <keras.src.engine.sequential.Sequential object at 0x7a36adeba380>>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n","\n","history = model.fit(encoded_train_processed, train_labels, epochs = 10, validation_split = 0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ePhsVsdRrvDQ","executionInfo":{"status":"ok","timestamp":1696887329753,"user_tz":180,"elapsed":144517,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"92391ad2-5339-40b3-eb7f-08130f9cc4be"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","105/105 [==============================] - 13s 103ms/step - loss: 0.2753 - acc: 0.9031 - val_loss: 0.1274 - val_acc: 0.9593\n","Epoch 2/10\n","105/105 [==============================] - 8s 77ms/step - loss: 0.0826 - acc: 0.9788 - val_loss: 0.0583 - val_acc: 0.9856\n","Epoch 3/10\n","105/105 [==============================] - 10s 93ms/step - loss: 0.0515 - acc: 0.9868 - val_loss: 0.0522 - val_acc: 0.9904\n","Epoch 4/10\n","105/105 [==============================] - 7s 71ms/step - loss: 0.0399 - acc: 0.9886 - val_loss: 0.0505 - val_acc: 0.9868\n","Epoch 5/10\n","105/105 [==============================] - 11s 103ms/step - loss: 0.0319 - acc: 0.9907 - val_loss: 0.0483 - val_acc: 0.9880\n","Epoch 6/10\n","105/105 [==============================] - 8s 81ms/step - loss: 0.0266 - acc: 0.9922 - val_loss: 0.0558 - val_acc: 0.9844\n","Epoch 7/10\n","105/105 [==============================] - 11s 100ms/step - loss: 0.0229 - acc: 0.9940 - val_loss: 0.0542 - val_acc: 0.9856\n","Epoch 8/10\n","105/105 [==============================] - 11s 102ms/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0588 - val_acc: 0.9880\n","Epoch 9/10\n","105/105 [==============================] - 9s 85ms/step - loss: 0.0176 - acc: 0.9958 - val_loss: 0.0653 - val_acc: 0.9821\n","Epoch 10/10\n","105/105 [==============================] - 11s 102ms/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0525 - val_acc: 0.9868\n"]}]},{"cell_type":"code","source":["results = model.evaluate(encoded_test_processed, test_labels)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0llSM1OtMIw","executionInfo":{"status":"ok","timestamp":1696887331934,"user_tz":180,"elapsed":1259,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"97953406-1814-4a00-c73c-06eaeece7682"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["44/44 [==============================] - 1s 16ms/step - loss: 0.0586 - acc: 0.9849\n","[0.058634236454963684, 0.9849137663841248]\n"]}]},{"cell_type":"code","source":["#to text new data, new setences, its necessary to encode again. Lets bring back our econding process\n","new_data = \"how are you doing today?\"\n","new_data = [new_data]\n","print(new_data)\n","\n","inputstr = tok.texts_to_sequences(new_data)\n","pred = model.predict(inputstr)\n","\n","# print(result_)\n","p = (pred[0][0])\n","print([p,(\"ham\" if p<0.5 else \"spam\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pT5eV3_4uET2","executionInfo":{"status":"ok","timestamp":1696887348218,"user_tz":180,"elapsed":634,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"a3ad2d78-6379-431d-b3cb-483c24ce2b3a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['how are you doing today?']\n","1/1 [==============================] - 1s 513ms/step\n","[0.0019632364, 'ham']\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"J9tD9yACG6M9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696887395638,"user_tz":180,"elapsed":235,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"cae4aab7-d76d-45fe-ca83-9aac9e170fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['how are you doing today?']\n","1/1 [==============================] - 0s 53ms/step\n","[0.0019632364, 'ham']\n"]}],"source":["# function to predict messages based on model\n","# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n","def predict_message(pred_text):\n","  new_data = pred_text\n","  new_data = [new_data]\n","  print(new_data)\n","\n","  inputstr = tok.texts_to_sequences(new_data)\n","  pred = model.predict(inputstr)\n","\n","  # print(result_)\n","  p = (pred[0][0])\n","  class_ = (\"ham\" if p<0.5 else \"spam\")\n","  # print([p,(\"ham\" if p<0.5 else \"spam\")])\n","\n","  return  [p, class_]\n","\n","pred_text = \"how are you doing today?\"\n","\n","prediction = predict_message(pred_text)\n","print(prediction)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Dxotov85SjsC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696887400730,"user_tz":180,"elapsed":1494,"user":{"displayName":"Gustavo Medeiros","userId":"15213961228767072598"}},"outputId":"437f510a-eabc-4283-a3c1-a2012773dced"},"outputs":[{"output_type":"stream","name":"stdout","text":["how are you doing today\n","['how are you doing today']\n","1/1 [==============================] - 0s 55ms/step\n","[0.0019632364, 'ham']\n","sale today! to stop texts call 98912460324\n","['sale today! to stop texts call 98912460324']\n","1/1 [==============================] - 0s 37ms/step\n","[0.81110317, 'spam']\n","i dont want to go. can we try it a different day? available sat\n","['i dont want to go. can we try it a different day? available sat']\n","1/1 [==============================] - 0s 476ms/step\n","[0.00047631626, 'ham']\n","our new mobile video service is live. just install on your phone to start watching.\n","['our new mobile video service is live. just install on your phone to start watching.']\n","1/1 [==============================] - 0s 54ms/step\n","[0.99658614, 'spam']\n","you have won £1000 cash! call to claim your prize.\n","['you have won £1000 cash! call to claim your prize.']\n","1/1 [==============================] - 0s 41ms/step\n","[0.9988115, 'spam']\n","i'll bring it tomorrow. don't forget the milk.\n","[\"i'll bring it tomorrow. don't forget the milk.\"]\n","1/1 [==============================] - 0s 37ms/step\n","[0.00044920386, 'ham']\n","wow, is your arm alright. that happened to me one time too\n","['wow, is your arm alright. that happened to me one time too']\n","1/1 [==============================] - 0s 44ms/step\n","[0.00034899745, 'ham']\n","You passed the challenge. Great job!\n"]}],"source":["# Run this cell to test your function and model. Do not modify contents.\n","def test_predictions():\n","  test_messages = [\"how are you doing today\",\n","                   \"sale today! to stop texts call 98912460324\",\n","                   \"i dont want to go. can we try it a different day? available sat\",\n","                   \"our new mobile video service is live. just install on your phone to start watching.\",\n","                   \"you have won £1000 cash! call to claim your prize.\",\n","                   \"i'll bring it tomorrow. don't forget the milk.\",\n","                   \"wow, is your arm alright. that happened to me one time too\"\n","                  ]\n","\n","  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n","  passed = True\n","\n","  for msg, ans in zip(test_messages, test_answers):\n","    print(msg)\n","    prediction = predict_message(msg)\n","    print(prediction)\n","    if prediction[1] != ans:\n","      passed = False\n","\n","  if passed:\n","    print(\"You passed the challenge. Great job!\")\n","  else:\n","    print(\"You haven't passed yet. Keep trying.\")\n","\n","test_predictions()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb","timestamp":1696812379421}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}